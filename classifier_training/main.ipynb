{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n",
    "# The pre-trained roberta-based detector may only works w/\n",
    "# - Huggingface version 2.9.1 (i.e., ```transformers==2.9.1```)\n",
    "# - ```tokenizers==0.7.0```\n",
    "# !pip install transformers==2.9.1\n",
    "\n",
    "'''\n",
    "~~~About Checkpoints~~~\n",
    "base.pt is the most accurate checkpoint\n",
    "base_1.pt is the latest checkpoint\n",
    "'''\n",
    "import numpy\n",
    "\n",
    "FROM_CHECKPOINT = False\n",
    "CHECKPOINTNAME = \"\"\n",
    "\n",
    "import os\n",
    "import csv\n",
    "import time\n",
    "import tqdm\n",
    "# from tqdm.notebook import trange\n",
    "from tqdm import trange\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from transformers import RobertaForSequenceClassification, RobertaTokenizer\n",
    "\n",
    "import utils as U\n",
    "\n",
    "import sys\n",
    "\n",
    "# setting path\n",
    "sys.path.append('..')\n",
    "\n",
    "from mutation_miniframework.Dataset import *\n",
    "from mutation_miniframework.operators import deleteRandomArticle, replaceLetters, replaceFromDictionary, \\\n",
    "\treplaceWordListWithRandomSelf\n",
    "\n",
    "misspellings = U.loadJSONWordDictionary(\"../mutation_miniframework/mutation_data/misspellings.json\")\n",
    "antonyms = U.loadJSONWordDictionary(\"../mutation_miniframework/mutation_data/antonyms.json\")\n",
    "synonyms = U.loadJSONWordDictionary(\"../mutation_miniframework/mutation_data/misspellings.json\")\n",
    "randomList = []\n",
    "with open(\"../mutation_miniframework/mutation_data/random_word.json\") as randomJSON:\n",
    "    randomBuffer = dict(json.load(randomJSON))\n",
    "    randomList = randomBuffer[\"word\"]\n",
    "\n",
    "project_data_path = \"./test\"\n",
    "\n",
    "text_data_path = os.path.join(project_data_path, 'data_10k', 'Parsed')\n",
    "human_text_dir = os.path.join(text_data_path, 'train_val_test/human')\n",
    "mutation_text_dir = os.path.join(text_data_path, 'train_val_test/mutation')\n",
    "synthetic_text_dir = os.path.join(text_data_path, 'train_val_test/synthetic')\n",
    "text_file_mutation = 'WikiMutationQuarterSet.json'\n",
    "text_file_human = 'WikiHumanQuarterSet.json'\n",
    "text_file_synthetic = 'WikiSyntheticQuarterSet.json'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "ckpt_dir = os.path.join(project_data_path, \"ckpt\")\n",
    "output_path = os.path.join(ckpt_dir, \"Ternary-Custom-Detector\")\n",
    "if(not os.path.exists(output_path)):\n",
    "    print(\"Making Dir...\\n\\t%s\" %output_path)\n",
    "    os.makedirs(output_path)\n",
    "\n",
    "roberta_detector_ckpt_dir = os.path.join(ckpt_dir, 'RoBERTa-Based-Detector')\n",
    "roberta_detector_ckpt_name = 'detector-large.pt'\n",
    "roberta_detector_ckpt_path = os.path.join(roberta_detector_ckpt_dir,\n",
    "                                          roberta_detector_ckpt_name)\n",
    "roberta_detector_ckpt_url = 'https://openaipublic.azureedge.net/gpt-2/detector-models/v1/detector-large.pt'\n",
    "\n",
    "# Download RoBERTa-based Detector ckpt if needed\n",
    "if (not os.path.exists(roberta_detector_ckpt_path)):\n",
    "    if(not os.path.exists(roberta_detector_ckpt_dir)):\n",
    "        print(\"Making Dir...\\n\\t%s\" %roberta_detector_ckpt_dir)\n",
    "        os.makedirs(roberta_detector_ckpt_dir)\n",
    "    U.download_roberta_ckpt(roberta_detector_ckpt_url,\n",
    "                            roberta_detector_ckpt_path)\n",
    "\n",
    "# Load data\n",
    "#[img name, captions, label]\n",
    "train_data = U.load_data(human_text_dir, text_file_human,\n",
    "                         mutation_text_dir, text_file_mutation,\n",
    "                         synthetic_text_dir, text_file_synthetic,\n",
    "                         train_test_split='train')\n",
    "val_data = U.load_data(human_text_dir, text_file_human,\n",
    "                       mutation_text_dir, text_file_mutation,\n",
    "                       synthetic_text_dir, text_file_synthetic,\n",
    "                       train_test_split='val')\n",
    "test_data = U.load_data(human_text_dir, text_file_human,\n",
    "                        mutation_text_dir, text_file_mutation,\n",
    "                        synthetic_text_dir, text_file_synthetic,\n",
    "                        train_test_split='test')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# set hyperparameters\n",
    "batch_size = 1\n",
    "epochs = 3\n",
    "learning_rate = 0.0001\n",
    "finetune_embeddings = False\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Initiate pre-trained RoBERTa-based Detector\n",
    "ckpt = None\n",
    "if FROM_CHECKPOINT is True:\n",
    "    ckpt = torch.load(os.path.join(output_path, CHECKPOINTNAME))\n",
    "else:\n",
    "    ckpt = torch.load(roberta_detector_ckpt_path) # checkpoint for pre-trained reberta detector, replace here with path.\n",
    "model = RobertaForSequenceClassification.from_pretrained('roberta-large')\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-large')\n",
    "\n",
    "if FROM_CHECKPOINT is True:\n",
    "    model.load_state_dict(ckpt)#Only do ckpt when loading\n",
    "else:\n",
    "    model.load_state_dict(ckpt['model_state_dict'])#Only do ckpt when loading\n",
    "model = model.to(device)\n",
    "model.classifier.out_proj = nn.Linear(1024, 3, bias=True)\n",
    "\n",
    "# Freeze roberta weights (i.e., the embedding weights)\n",
    "# leave the classifier tunable\n",
    "for p in model.roberta.parameters():\n",
    "    p.requires_grad = finetune_embeddings\n",
    "\n",
    "\n",
    "\n",
    "BCE = nn.BCELoss()\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "\n",
    "## Train Loop ##\n",
    "t = trange(epochs, desc=\"\", position=0, leave=True)\n",
    "\n",
    "phases = [\"train\", \"val\"]\n",
    "best_val_acc = 0\n",
    "best_epoch = 0\n",
    "train_hist = []\n",
    "val_hist = []"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch/Step: 0/385[Phase:train]  Loss:1.8712  CorrectPred:0.5130 [198/386]:   0%|          | 0/3 [00:35<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "from random import randrange\n",
    "for e in t:\n",
    "    model.to(device)\n",
    "    for phase in phases:\n",
    "\n",
    "        ## Initialization ##\n",
    "        if phase == \"train\":\n",
    "            model.train()\n",
    "            data = np.array(train_data)\n",
    "            np.random.shuffle(data)\n",
    "        else:\n",
    "            model.eval()\n",
    "            data = np.array(val_data)\n",
    "\n",
    "        epoch_loss = 0\n",
    "        epoch_correct_pred = 0\n",
    "        step_per_epoch = math.floor(len(data) / batch_size)\n",
    "\n",
    "        ## Train/Val Loop ##\n",
    "        for i in range(step_per_epoch):\n",
    "            # Load one batch of data\n",
    "            # batch size might have to be 1 due to varying caption length\n",
    "            cur_data = data[i*batch_size:(i+1)*batch_size]\n",
    "            cur_names = cur_data[:,0]\n",
    "            cur_captions = cur_data[:,1]#TODO: Adjust for extra dimension, captions and labels\n",
    "            cur_labels = cur_data[:,2].astype(np.int8)#Mutation is 0, real is 1, synthetic is 2\n",
    "\n",
    "            # Generate mutation\n",
    "            # if need to generate mutation, add code here\n",
    "            # print(\"DATA \" + str(cur_data))\n",
    "            dataDict = {}\n",
    "            if(phase == \"train\" and cur_labels == 0):\n",
    "                dataDict[0] = [str(cur_data[:,1])]\n",
    "                choice = randrange(0, 6)\n",
    "                mutatedCaptionData = Dataset(dataDict, [\"\"])\n",
    "                if choice == 0:\n",
    "                    deleteRandomArticle(mutatedCaptionData, [\" a \", \" an \", \" the \"], \"\", word_change_limit=99)\n",
    "                if choice == 1:\n",
    "                    replaceLetters(mutatedCaptionData, {\n",
    "                        \"a\": \"α\",\n",
    "                        \"e\": \"ε\"\n",
    "                    }, \"\", word_change_limit=3)\n",
    "                if choice == 2:\n",
    "                    replaceFromDictionary(mutatedCaptionData, misspellings, \"\", word_change_limit=3)\n",
    "                if choice == 3:#random word replacement\n",
    "                    replaceWordListWithRandomSelf(mutatedCaptionData, randomList, \"\", word_change_limit=2)\n",
    "                if choice == 4:#synonyms replacement\n",
    "                    replaceFromDictionary(mutatedCaptionData, synonyms, \"\", word_change_limit=2)\n",
    "                if choice == 5:#antonyms replacement\n",
    "                    replaceFromDictionary(mutatedCaptionData, antonyms, \"\", word_change_limit=2)\n",
    "                if choice == 6:\n",
    "                    pass\n",
    "                # print(\"MUTATION \" + mutatedCaptionData[0][0])\n",
    "                cur_data[:,1] = mutatedCaptionData[0][0]\n",
    "                cur_captions = cur_data[:,1]\n",
    "            # print(str(choice) + \":\" + str(cur_data[:,1][0]))\n",
    "\n",
    "            # Tokenize captions\n",
    "            cur_token_ids = [tokenizer.encode(item) for item in cur_captions]\n",
    "            cur_masks = [np.ones(len(item)) for item in cur_token_ids]\n",
    "\n",
    "            # Convert to tensor and send data to device\n",
    "            cur_token_ids = torch.tensor(np.array(cur_token_ids)).to(device)\n",
    "            cur_labels = torch.tensor(np.array(cur_labels)).long().to(device)\n",
    "            cur_masks = torch.tensor(np.array(cur_masks)).to(device)\n",
    "\n",
    "            # For training\n",
    "            if(phase == \"train\"):\n",
    "                optimizer.zero_grad()\n",
    "                logits = model(cur_token_ids, attention_mask=cur_masks)\n",
    "                loss = loss_function(logits[0], cur_labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            # scheduler may be needed in the future\n",
    "\n",
    "            # For validation\n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    logits = model(cur_token_ids, attention_mask=cur_masks)\n",
    "                    loss = loss_function(logits[0], cur_labels)\n",
    "\n",
    "            # Track current performance\n",
    "            # Count correct prediciton\n",
    "            for kk in range(len(cur_labels)):\n",
    "                prob = logits[0][kk].softmax(dim=-1)\n",
    "                pred = torch.argmax(prob.detach().cpu())\n",
    "                if(pred==cur_labels[kk]):\n",
    "                    epoch_correct_pred +=  1.0\n",
    "            # Add current loss to total epoch loss\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            # Update progress bar\n",
    "            t.set_description(\"Epoch/Step: %i/%i[Phase:%s]  Loss:%.4f  CorrectPred:%.4f [%i/%i]\"\n",
    "                              % (e, i, phase, loss.item(), epoch_correct_pred/(i+1), epoch_correct_pred, (i+1)))\n",
    "\n",
    "\n",
    "        ## Compute epoch performance ##\n",
    "        epoch_acc = epoch_correct_pred / (step_per_epoch * batch_size)\n",
    "        epoch_loss = epoch_loss / step_per_epoch\n",
    "\n",
    "        if(phase==\"train\"):\n",
    "            train_hist.append([epoch_loss, epoch_acc])\n",
    "            np.save(os.path.join(output_path,\"train_hist.npy\"),\n",
    "                    np.asarray(train_hist))\n",
    "\n",
    "        else:\n",
    "            val_hist.append([epoch_loss, epoch_acc])\n",
    "            np.save(os.path.join(output_path,\"val_hist.npy\"),\n",
    "                    np.asarray(val_hist))\n",
    "\n",
    "        if(phase == \"val\"):\n",
    "            if(epoch_acc>best_val_acc):\n",
    "                best_val_acc = epoch_acc\n",
    "                best_epoch = e\n",
    "                print(\"Epoch:%d Acc:%.4f higher than the previous best performance\"\n",
    "                      %(e, best_val_acc))\n",
    "                print(\"Saving ckpt...\")\n",
    "                # save the CKPT\n",
    "                torch.save(model.cpu().state_dict(),\n",
    "                           os.path.join(output_path,\"base.pt\"))\n",
    "\n",
    "    print(\"\\nEpoch:%d   Train Loss/Acc: %.4f/%.4f   Val Loss/ACC %.4f/%.4f\"\n",
    "          %(e, train_hist[e][0], train_hist[e][1], val_hist[e][0], val_hist[e][1]))\n",
    "\n",
    "torch.save(model.cpu().state_dict(),\n",
    "           os.path.join(output_path,\"base_\"+str(e)+\".pt\"))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.ticker as mtick\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ((ax1, ax2)) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "ax1.plot([train_hist[e][1] for e in range(epochs)])\n",
    "ax1.yaxis.set_major_formatter(mtick.PercentFormatter())\n",
    "ax1.set_xticks([x for x in range(0, len(epochs), 1)])\n",
    "ax1.set_xlabel(\"Epoch\")\n",
    "ax1.set_ylabel(\"Accuracy(%)\")\n",
    "ax2.plot([val_hist[e][1] for e in range(epochs)])\n",
    "ax2.set_xticks([x for x in range(0, len(epochs), 1)])\n",
    "ax2.set_xlabel(\"Epoch\")\n",
    "ax2.set_ylabel(\"Accuracy(%)\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
